# üü¶ BLOQUE 4 ‚Äî √âtica y Legalidad en la Inteligencia Artificial

## üéØ Objetivo global

Concienciar al estudiante sobre:  
‚úÖ los riesgos y desaf√≠os sociales de la IA  
‚úÖ principios √©ticos para el desarrollo responsable  
‚úÖ el marco legal emergente en la UE y el mundo  
‚úÖ el impacto de la IA sobre derechos fundamentales

---

## üìå Ejes tem√°ticos principales (seg√∫n materiales y presentaciones)

### 1Ô∏è‚É£ Principios √©ticos en IA

- Transparencia (explicabilidad de los modelos)  
- Justicia y equidad (evitar sesgos y discriminaci√≥n)  
- Responsabilidad (accountability, trazabilidad de decisiones)  
- Privacidad y protecci√≥n de datos  
- Beneficio social (alineaci√≥n con valores humanos)  
- No maleficencia (evitar da√±o intencionado o no intencionado)

üëâ **Prioridad alta**: el profesor dijo que podr√≠an preguntar ejemplos pr√°cticos de estos principios.

---

### 2Ô∏è‚É£ Riesgos sociales y tecnol√≥gicos de la IA

- Discriminaci√≥n algor√≠tmica (por sesgos en datos o dise√±o)  
- Exclusi√≥n social (falta de acceso a tecnolog√≠as)  
- Desplazamiento de empleo  
- Vigilancia masiva y p√©rdida de privacidad  
- Dependencia tecnol√≥gica excesiva  
- Da√±os indirectos (uso indebido, ciberataques)

üëâ **Prioridad media/alta**: aparece en preguntas de discusi√≥n.

---

### 3Ô∏è‚É£ Regulaci√≥n y marcos legales

- Normativa de protecci√≥n de datos (GDPR en Europa)  
- AI Act europeo (propuesta de regulaci√≥n de sistemas de IA por niveles de riesgo)  
- Derechos digitales  
- Leyes sobre responsabilidad civil por fallos de IA  
- Marco de la UNESCO sobre √©tica de la IA  
- Regulaci√≥n sectorial (sanidad, finanzas, transporte, etc.)

üëâ **Prioridad alta**: especialmente el **AI Act** europeo y su clasificaci√≥n de riesgos, tal como subray√≥ el profesor.

---

### 4Ô∏è‚É£ Gobernanza de la IA

- Auditor√≠as algor√≠tmicas  
- Evaluaciones de impacto √©tico y social  
- Mecanismos de supervisi√≥n humana  
- Est√°ndares y certificaciones de IA responsable  
- Pol√≠ticas corporativas y c√≥digos de conducta


üëâ Prioridad media: puede entrar como complemento de la parte legal.
---

### 5Ô∏è‚É£ Explicabilidad

- Capacidad de entender c√≥mo y por qu√© un modelo toma decisiones  
- Es crucial para contextos cr√≠ticos (medicina, coches aut√≥nomos, justicia)  
- Relaci√≥n con la confianza del usuario:  
  - modelos explicables = mayor confianza  
  - modelos opacos = menor confianza  
- Dilema eficacia vs. explicabilidad: los modelos m√°s potentes (deep learning) son los menos explicables  
- **Caja blanca** (transparente) vs. **caja negra** (opaco)  
- Ejemplos donde se necesita explicabilidad:  
  - ¬øPor qu√© me deniegan un pr√©stamo?  
  - ¬øPor qu√© un coche frena?  
  - ¬øPor qu√© me recomiendan un tratamiento m√©dico?

---

## üü¶ Cruce con priorizaci√≥n seg√∫n las transcripciones

El profesor insisti√≥ en:

‚úÖ **Preguntas abiertas**  
- Ejemplos de principios √©ticos aplicados a casos reales  
- Explicaci√≥n del AI Act europeo y clasificaci√≥n de riesgos  
- Posibles impactos sociales de la IA (positivo y negativo)

‚úÖ **Pregunta larga (memoria de investigaci√≥n)**  
- Podr√≠a incluir un apartado de ‚Äúimpacto √©tico y legal‚Äù al proponer un proyecto IA (por ejemplo, evaluar privacidad, sesgos y explicabilidad).

---

# ‚úÖ Conclusi√≥n de prioridades para tu estudio (BLOQUE 4)

‚úÖ Enf√≥cate en:  
‚úîÔ∏è definir principios √©ticos y aplicarlos a ejemplos reales  
‚úîÔ∏è entender los riesgos sociales y tecnol√≥gicos  
‚úîÔ∏è conocer el marco normativo europeo (AI Act, GDPR)  
‚úîÔ∏è saber plantear auditor√≠as y mecanismos de gobernanza  
‚úîÔ∏è reflexionar sobre el equilibrio entre innovaci√≥n y derechos fundamentales

---

# üü¶ Bloque 4 ‚Äî √âtica y Legalidad en la Inteligencia Artificial (resumen por subtemas)

---

## 1Ô∏è‚É£ Principios √©ticos en IA

La inteligencia artificial debe desarrollarse de forma responsable, siguiendo principios √©ticos:  
- **Transparencia:** permitir explicar las decisiones de los modelos de IA.  
- **Justicia y equidad:** evitar sesgos y discriminaci√≥n hacia grupos vulnerables.  
- **Responsabilidad (accountability):** establecer qui√©n responde por fallos o da√±os causados.  
- **Privacidad y protecci√≥n de datos:** respetar los derechos de las personas sobre su informaci√≥n.  
- **Beneficio social:** priorizar proyectos que aporten valor y bienestar a la sociedad.  
- **No maleficencia:** prevenir posibles da√±os intencionados o no intencionados.

üëâ Estos principios gu√≠an tanto la investigaci√≥n como la implantaci√≥n pr√°ctica de la IA.

---

## 2Ô∏è‚É£ Riesgos sociales y tecnol√≥gicos de la IA

La IA puede generar efectos no deseados si no se gestiona bien:  
- **Discriminaci√≥n algor√≠tmica** por sesgos en datos o modelos  
- **Exclusi√≥n social** por brechas de acceso a tecnolog√≠as  
- **Desplazamiento de empleos** debido a la automatizaci√≥n  
- **Vigilancia masiva** con p√©rdida de privacidad  
- **Dependencia tecnol√≥gica** que reduce la autonom√≠a humana  
- **Ciberataques** y da√±os indirectos por usos indebidos

üëâ Comprender estos riesgos es clave para prevenirlos y mitigarlos.

---

## 3Ô∏è‚É£ Regulaci√≥n y marcos legales

El avance de la IA ha motivado nuevas regulaciones:  
- **GDPR europeo:** regula la protecci√≥n de datos personales.  
- **AI Act europeo:** clasifica los sistemas de IA seg√∫n su nivel de riesgo (alto, limitado, m√≠nimo, prohibido) y establece requisitos seg√∫n el nivel.  
- **Derechos digitales:** proteger la autonom√≠a y privacidad de las personas.  
- **Responsabilidad civil:** reglas para asumir da√±os causados por IA defectuosa.  
- **Marco de la UNESCO:** orientaciones √©ticas internacionales.  
- **Regulaci√≥n sectorial:** normas espec√≠ficas para IA en sanidad, transporte, finanzas, etc.

üëâ Prioritario conocer el **AI Act** y su clasificaci√≥n de riesgos.

---

## 4Ô∏è‚É£ Gobernanza de la IA

La gobernanza busca asegurar un uso seguro, justo y responsable de la IA:  
- **Auditor√≠as algor√≠tmicas** para detectar sesgos o fallos  
- **Evaluaciones de impacto √©tico** antes de desplegar sistemas  
- **Supervisi√≥n humana** sobre decisiones autom√°ticas  
- **Est√°ndares y certificaciones** de calidad y seguridad  
- **C√≥digos de conducta corporativos** que garanticen principios √©ticos en toda la organizaci√≥n

üëâ Son mecanismos complementarios al marco legal para una IA confiable.

---

# ‚úÖ Resumen final de prioridades (recordatorio)

‚úÖ Para el examen prioriza:  
‚úîÔ∏è recordar y aplicar los principios √©ticos a ejemplos concretos  
‚úîÔ∏è conocer los principales riesgos sociales y t√©cnicos  
‚úîÔ∏è entender la regulaci√≥n europea (AI Act, GDPR)  
‚úîÔ∏è explicar auditor√≠as y procesos de gobernanza  
‚úîÔ∏è reflexionar sobre la compatibilidad entre innovaci√≥n y derechos fundamentales

---

# üìù Preguntas tipo examen ‚Äî Bloque 4

---

## **Pregunta 1**

**Enumere y explique brevemente al menos cuatro principios √©ticos que deber√≠an aplicarse en el desarrollo de sistemas de IA.**

‚úÖ **Posible respuesta:**

- **Transparencia:** las decisiones de la IA deben poder explicarse y auditarse.  
- **Equidad:** evitar sesgos que discriminen a grupos sociales.  
- **Responsabilidad:** garantizar que haya personas responsables de supervisar el sistema.  
- **Privacidad:** proteger los datos personales y limitar su uso solo a lo necesario.  
- **No maleficencia:** prevenir da√±os o consecuencias negativas no intencionadas.

---

## **Pregunta 2**

**Describa dos riesgos sociales que podr√≠an derivarse de un mal uso de la IA, e indique posibles medidas de mitigaci√≥n.**

‚úÖ **Posible respuesta:**

- **Discriminaci√≥n algor√≠tmica:** mitigar realizando auditor√≠as de datos y modelos para identificar sesgos.  
- **Desplazamiento laboral:** mitigar con pol√≠ticas de reciclaje profesional y formaci√≥n continua.

---

## **Pregunta 3**

**Explique qu√© es el AI Act europeo y c√≥mo clasifica los sistemas de IA.**

‚úÖ **Posible respuesta:**

El AI Act europeo es una propuesta regulatoria que busca garantizar el uso seguro y √©tico de la IA. Clasifica los sistemas en cuatro niveles de riesgo:  
- **Riesgo m√≠nimo** (sin restricciones)  
- **Riesgo limitado** (obligaci√≥n de transparencia)  
- **Riesgo alto** (sujeto a requisitos estrictos de control, auditor√≠a y documentaci√≥n)  
- **Riesgo prohibido** (sistemas de IA vetados por atentar contra derechos fundamentales)

---

## **Pregunta 4**

**Cite y explique dos mecanismos de gobernanza que pueden aplicarse a proyectos de IA.**

‚úÖ **Posible respuesta:**

- **Auditor√≠as algor√≠tmicas:** revisi√≥n independiente de modelos para detectar sesgos, errores o incumplimientos normativos.  
- **Supervisi√≥n humana:** intervenci√≥n de personas para supervisar y aprobar decisiones cr√≠ticas generadas por sistemas autom√°ticos.

---

## **Pregunta 5**

**Analice brevemente por qu√© la IA puede representar un riesgo para la privacidad.**

‚úÖ **Posible respuesta:**

La IA procesa grandes cantidades de datos personales, a veces sin transparencia suficiente, lo que puede implicar:  
- vulneraci√≥n de la intimidad  
- perfiles de personas sin su consentimiento  
- vigilancia masiva  
Para mitigar estos riesgos es clave aplicar el principio de minimizaci√≥n de datos y cumplir normativas como el GDPR.

---

## **Pregunta 6 (caso pr√°ctico largo)**

**Plantee el apartado √©tico de un proyecto de IA que automatiza procesos de selecci√≥n de personal. Incluya los principios aplicados, riesgos identificados y mecanismos de control.**

‚úÖ **Posible respuesta:**

- **Principios aplicados:**  
  - Equidad: evitar discriminaci√≥n por g√©nero, edad, raza  
  - Transparencia: explicar los criterios de selecci√≥n  
  - Responsabilidad: supervisi√≥n humana de las decisiones finales  

- **Riesgos identificados:**  
  - Sesgos ocultos en los datos hist√≥ricos de contrataci√≥n  
  - Exclusi√≥n de grupos minoritarios  
  - Falta de explicabilidad del modelo  

- **Mecanismos de control:**  
  - Auditor√≠as peri√≥dicas de sesgos  
  - Panel de revisi√≥n √©tica  
  - Registro de decisiones autom√°ticas con justificaci√≥n accesible

---
